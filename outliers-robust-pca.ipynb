{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "import tflib as lib\n",
    "import tflib.save_images\n",
    "import tflib.plot\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from keras_radam.training import RAdamOptimizer\n",
    "\n",
    "\n",
    "LAMBDA = 1000.0 # Gradient penalty lambda hyperparameter --- it regulates how strongly we enforce lipschitzness\n",
    "LAMBDA_2 = 5.0 # Constant for consistency term as in https://openreview.net/pdf?id=SJx9GQb0-\n",
    "CRITIC_ITERS = 5 # How many critic iterations per generator iteration\n",
    "BATCH_SIZE = 40 # Batch size\n",
    "ITERS = 3000 # How many generator iterations to train for\n",
    "num_epochs = 100 # number of iterations of alternating scheme\n",
    "OUTPUT_DIM = 400 # output dimension\n",
    "Lambda = 100.0 # the main lambda --- constant in front of the second term that enforces low-dimensionality of support \n",
    "Dimension = 10 # Hidden dimensionality of data --- rank of matrix L\n",
    "GAMMA = 1/OUTPUT_DIM # parameter of gaussian kernel M(x,y) = exp(-GAMMA*(x-y)^2)\n",
    "delta = 0.1 # regulates percentage of outliers\n",
    "MODE = 'neutral' # case I - neutral, case II-something else\n",
    "\n",
    "SIZE_COV = 10000*OUTPUT_DIM # how many times we sample fake and real to estimate matrix M_Ñ„ from step (**)\n",
    "TIMES_COV = int(SIZE_COV/BATCH_SIZE)\n",
    "SIZE_COV = TIMES_COV*BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I generate matrix X\n",
    "num = 400\n",
    "A = np.random.normal(loc=0.0, scale=1.0, size=(int(num*(1-delta)), Dimension))\n",
    "B = np.random.normal(loc=0.0, scale=1.0, size=(Dimension, OUTPUT_DIM))\n",
    "if MODE == 'neutral':\n",
    "    C = np.random.normal(loc=0.0, scale=1.0, size=(int(num*delta), OUTPUT_DIM))\n",
    "else:\n",
    "    C = np.tile(np.random.normal(loc=0.0, scale=1.0, size=(1, OUTPUT_DIM)), (int(num*delta), 1))\n",
    "images = np.concatenate((np.matmul(A, B), C), axis = 0)\n",
    "print(images.shape)\n",
    "u, s, vh = np.linalg.svd(np.matmul(A, B), full_matrices=True)\n",
    "D = vh[0:Dimension]\n",
    "P_correct = np.matmul(np.transpose(D), D)\n",
    "\n",
    "u, s, vh = np.linalg.svd(images, full_matrices=True)\n",
    "D = vh[0:Dimension]\n",
    "P_svd = np.matmul(np.transpose(D), D)\n",
    "DIFF = P_svd - P_correct\n",
    "dist = np.sqrt(np.sum(DIFF*DIFF))\n",
    "print(\"Frobenius distance from SVD till correct %0.4f\" %dist)\n",
    "\n",
    "L1 = np.sum(np.sqrt(np.sum(np.square(images-np.matmul(images, P_correct)), axis=1)))\n",
    "print(\"L12 distance till correct %0.4f\" %L1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_vars_from_scope (scope = None):\n",
    "    ret = []\n",
    "    for var in tf.global_variables():\n",
    "        if re.search(scope, var.name):\n",
    "            ret.append(var)\n",
    "    return ret\n",
    "\n",
    "init_scale = 0.05\n",
    "highway_size = OUTPUT_DIM\n",
    "def highway_layer(highway_inputs):\n",
    "    transf_weights = tf.get_variable('transf_weights', \n",
    "        [highway_size, highway_size],\n",
    "        initializer=tf.zeros_initializer(),\n",
    "        dtype=tf.float32)\n",
    "    transf_biases = tf.get_variable('transf_biases', [highway_size],\n",
    "        initializer=tf.zeros_initializer(),\n",
    "        dtype=tf.float32)\n",
    "    highw_weights = tf.get_variable('highw_weights', \n",
    "        [highway_size, highway_size],\n",
    "        initializer=tf.initializers.identity(),\n",
    "        dtype=tf.float32)\n",
    "    highw_biases = tf.get_variable('highw_biases', [highway_size],\n",
    "        initializer=tf.zeros_initializer(),\n",
    "        dtype=tf.float32)\n",
    "    transf_gate = tf.nn.sigmoid(tf.matmul(highway_inputs, transf_weights)         + transf_biases)\n",
    "    highw_output = tf.multiply(transf_gate, \n",
    "        tf.nn.relu(tf.matmul(highway_inputs, highw_weights) + highw_biases)) \\\n",
    "        + tf.multiply(tf.ones([highway_size], dtype=tf.float32) - transf_gate, \n",
    "        highway_inputs)\n",
    "    return highw_output\n",
    "\n",
    "def Generator(n_samples, noise=None):\n",
    "    if noise is None:\n",
    "        noise = tf.random.uniform(shape=[n_samples], minval=0, maxval=OUTPUT_DIM, dtype=tf.dtypes.int32)\n",
    "        output = tf.one_hot(noise, OUTPUT_DIM, axis=-1)\n",
    "        noise1 = tf.random_normal([n_samples, OUTPUT_DIM])\n",
    "    with tf.variable_scope('Generator', reuse=tf.AUTO_REUSE) as scope:\n",
    "        output = tf.layers.dense(output, OUTPUT_DIM, activation=None)\n",
    "        output1 = tf.layers.dense(noise1, OUTPUT_DIM, activation=tf.nn.tanh)\n",
    "        output1 = 10*tf.layers.dense(output1, OUTPUT_DIM, activation=tf.nn.tanh)\n",
    "    return tf.reshape(output+output1, [-1, OUTPUT_DIM])\n",
    "\n",
    "def Generator_retro(n_samples, noise=None):\n",
    "    if noise is None:\n",
    "        noise = tf.random.uniform(shape=[n_samples], minval=0, maxval=OUTPUT_DIM, dtype=tf.dtypes.int32)\n",
    "        output = tf.one_hot(noise, OUTPUT_DIM, axis=-1)\n",
    "        noise1 = tf.random_normal([n_samples, OUTPUT_DIM])\n",
    "    with tf.variable_scope('Retro', reuse=tf.AUTO_REUSE) as scope:\n",
    "        output = tf.layers.dense(output, OUTPUT_DIM, activation=None)\n",
    "        output1 = tf.layers.dense(noise1, OUTPUT_DIM, activation=tf.nn.tanh)\n",
    "        output1 = 10*tf.layers.dense(output1, OUTPUT_DIM, activation=tf.nn.tanh)\n",
    "    return tf.reshape(output+output1, [-1, OUTPUT_DIM])\n",
    "\n",
    "def Copy_generators():\n",
    "    i=0\n",
    "    for h in gen_params:\n",
    "        tf.assign(gen_params_retro[i], h)\n",
    "        i=i+1\n",
    "\n",
    "\n",
    "def Discriminator(inputs):\n",
    "    output = tf.reshape(inputs, [-1, OUTPUT_DIM])\n",
    "    \n",
    "    with tf.variable_scope('Discriminator', reuse=tf.AUTO_REUSE) as scope:\n",
    "        with tf.variable_scope('highway1', reuse=tf.AUTO_REUSE):\n",
    "            output = highway_layer(output)\n",
    "        with tf.variable_scope('highway2', reuse=tf.AUTO_REUSE):\n",
    "            output = highway_layer(output)\n",
    "        output = tf.nn.dropout(output, keep_prob=0.50)\n",
    "        with tf.variable_scope('highway3', reuse=tf.AUTO_REUSE):\n",
    "            output = highway_layer(output)\n",
    "        with tf.variable_scope('highway4', reuse=tf.AUTO_REUSE):\n",
    "            output = highway_layer(output)\n",
    "        output = tf.nn.dropout(output, keep_prob=0.50)\n",
    "        with tf.variable_scope('highway5', reuse=tf.AUTO_REUSE):\n",
    "            output = highway_layer(output)\n",
    "        with tf.variable_scope('highway6', reuse=tf.AUTO_REUSE):\n",
    "            output = highway_layer(output)\n",
    "        output = tf.math.sqrt(tf.reduce_sum(tf.square(output), axis=1)+0.0001)\n",
    "    return tf.reshape(output, [-1])\n",
    "\n",
    "P = tf.placeholder(tf.float32, shape=[OUTPUT_DIM, Dimension])\n",
    "real_data = tf.placeholder(tf.float32, shape=[BATCH_SIZE, OUTPUT_DIM])\n",
    "fake_data = Generator(BATCH_SIZE)\n",
    "fake_data2 = Generator(BATCH_SIZE)\n",
    "disc_real = Discriminator(real_data)\n",
    "disc_real_= Discriminator(real_data)\n",
    "disc_fake = Discriminator(fake_data)\n",
    "\n",
    "gen_params = get_vars_from_scope('Generator')\n",
    "disc_params = get_vars_from_scope('Discriminator')\n",
    "\n",
    "\n",
    "fake_data_retro = Generator_retro(BATCH_SIZE)\n",
    "gen_params_retro = get_vars_from_scope('Retro')\n",
    "\n",
    "gen_cost = -tf.reduce_mean(disc_fake)\n",
    "disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)\n",
    "\n",
    "#Consistency term from https://openreview.net/pdf?id=SJx9GQb0-\n",
    "CT = LAMBDA_2*tf.square(disc_real-disc_real_)\n",
    "CT_ = tf.maximum(CT-0.0,0.0*CT)\n",
    "disc_cost += tf.reduce_mean(CT_)\n",
    "\n",
    "# Gradient penalty\n",
    "alpha = tf.random_uniform(shape=[BATCH_SIZE,1], minval=0.,maxval=1.)\n",
    "differences = fake_data - real_data\n",
    "interpolates = real_data + (alpha*differences)\n",
    "gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]\n",
    "\n",
    "slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "gradient_penalty = tf.reduce_mean(tf.clip_by_value(slopes - 1.0, 0, np.infty)**2)\n",
    "\n",
    "disc_cost += LAMBDA*gradient_penalty\n",
    "\n",
    "# I needed to know how strongly \"leakage\" can blow-up\n",
    "leakage = LAMBDA*gradient_penalty \n",
    "\n",
    "Q = tf.reshape(fake_data, [BATCH_SIZE,-1])\n",
    "Q2 = tf.reshape(fake_data2, [BATCH_SIZE,-1])\n",
    "Gramm = tf.matmul(Q,tf.transpose(Q2))\n",
    "R = tf.reshape(tf.reduce_sum(tf.multiply(Q,Q), axis=1), [BATCH_SIZE,1])\n",
    "R2 = tf.reshape(tf.reduce_sum(tf.multiply(Q2,Q2), axis=1), [BATCH_SIZE,1])\n",
    "t1 = tf.ones([BATCH_SIZE, 1])\n",
    "t2 = tf.ones([1, BATCH_SIZE])\n",
    "S = tf.multiply(tf.exp(GAMMA*(-tf.matmul(R, t2) - tf.matmul(t1, tf.transpose(R2)) + 2*Gramm)), Gramm)\n",
    "\n",
    "Q_retro = tf.reshape(fake_data_retro, [BATCH_SIZE,-1])\n",
    "R_retro = tf.reshape(tf.reduce_sum(tf.multiply(Q_retro,Q_retro), axis=1), [BATCH_SIZE,1])\n",
    "Gramm2 = tf.matmul(Q,tf.transpose(Q_retro))\n",
    "Q3 = tf.matmul(tf.matmul(Q_retro, P), tf.transpose(P))\n",
    "Gramm3 = tf.matmul(Q,tf.transpose(Q3))\n",
    "S2 = tf.multiply(tf.exp(GAMMA*(-tf.matmul(R, t2) - tf.matmul(t1, tf.transpose(R_retro)) + 2*Gramm2)), Gramm3)\n",
    "\n",
    "gen_cost += Lambda*tf.reduce_mean(S) - 2*Lambda*tf.reduce_mean(S2)\n",
    "\n",
    "total = tf.reduce_mean(disc_real) - tf.reduce_mean(disc_fake) + Lambda*tf.reduce_mean(S) - 2*Lambda*tf.reduce_mean(S2)\n",
    "\n",
    "\n",
    "# Radam converges faster than Adam, so I use it\n",
    "optimizer_gen = RAdamOptimizer(learning_rate=1e-5, beta1=0.5, beta2=0.9)\n",
    "optimizer_disc = RAdamOptimizer(learning_rate=1e-5, beta1=0.5, beta2=0.9)\n",
    "gen_train_op = optimizer_gen.minimize(gen_cost, var_list=gen_params)\n",
    "disc_train_op = optimizer_disc.minimize(disc_cost, var_list=disc_params)\n",
    "\n",
    "\n",
    "# Also, weights of discriminator are clipped as in original paper of Arjovsky et al, \n",
    "# though this is not needed in fact \n",
    "clip_ops = []\n",
    "for var in disc_params:\n",
    "    clip_bounds = [-10.0, 10.0]\n",
    "    clip_ops.append(\n",
    "        tf.assign(\n",
    "            var, \n",
    "            tf.clip_by_value(var, clip_bounds[0], clip_bounds[1])\n",
    "        )\n",
    "    )\n",
    "clip_disc_weights = tf.group(*clip_ops)\n",
    "\n",
    "\n",
    "num = images.shape[0]\n",
    "images = np.reshape(images, (num, -1))\n",
    "data = np.random.permutation(images)\n",
    "\n",
    "# Train loop\n",
    "P_retro = np.zeros((OUTPUT_DIM,Dimension))\n",
    "current = 0\n",
    "c_size = int(num/BATCH_SIZE)\n",
    "ft1 = np.ones([BATCH_SIZE, 1])\n",
    "ft2 = np.ones([1, BATCH_SIZE])\n",
    "needed_vars = gen_params + disc_params\n",
    "saver = tf.train.Saver(var_list=needed_vars)\n",
    "for epoch in range(num_epochs):\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.initialize_all_variables())\n",
    "        if(epoch>0):\n",
    "            saver.restore(session, \"/tmp/model.ckpt\")\n",
    "            i=0\n",
    "            for h in gen_params:\n",
    "                gen_params_retro[i].load(session.run(h), session)\n",
    "                i=i+1\n",
    "        for iteration in range(ITERS):\n",
    "            start_time = time.time()\n",
    "            # Train generator\n",
    "            if iteration > 0:\n",
    "                start_index = current % c_size\n",
    "                start_index = start_index*BATCH_SIZE\n",
    "                _data = data[start_index:start_index+BATCH_SIZE]\n",
    "                _ = session.run(gen_train_op, feed_dict={real_data: _data, P: P_retro})\n",
    "                current = current+1\n",
    "            # Train critic\n",
    "            disc_iters = CRITIC_ITERS\n",
    "            for i in range(disc_iters):\n",
    "                start_index = current % c_size\n",
    "                start_index = start_index*BATCH_SIZE\n",
    "                _data = data[start_index:start_index+BATCH_SIZE]\n",
    "                _disc_cost, _ = session.run([disc_cost, disc_train_op], feed_dict={real_data: _data, P: P_retro})\n",
    "                _ = session.run(clip_disc_weights)\n",
    "                current = current+1\n",
    "\n",
    "            lib.plot.plot('train disc cost', _disc_cost)\n",
    "            lib.plot.plot('time', time.time() - start_time)\n",
    "\n",
    "            # Calculate dev loss and generate samples every 500 iters\n",
    "            if (epoch*ITERS+iteration) % 500 == 499 or iteration == 0:\n",
    "                dev_disc_costs = []\n",
    "                dev_total = []\n",
    "                dev_leakage = []\n",
    "                for image in range(c_size):\n",
    "                    start_index = image*BATCH_SIZE\n",
    "                    _dev_disc_cost, _total, _leakage = session.run([disc_cost, total, leakage], feed_dict={real_data: \n",
    "                                                    data[start_index:start_index+BATCH_SIZE], P: P_retro}) \n",
    "                    dev_disc_costs.append(_dev_disc_cost)\n",
    "                    dev_total.append(_total)\n",
    "                    dev_leakage.append(_leakage)\n",
    "                lib.plot.plot('dev disc cost', np.mean(dev_disc_costs))\n",
    "                lib.plot.plot('dev total cost', np.mean(dev_total))\n",
    "                lib.plot.plot('dev leakage cost', np.mean(dev_leakage))\n",
    "\n",
    "            # Save logs every 100 iters\n",
    "            if (iteration % 100 == 99): #(iteration < 2) or \n",
    "                lib.plot.flush()\n",
    "\n",
    "            lib.plot.tick()\n",
    "        M = np.zeros((OUTPUT_DIM,OUTPUT_DIM))\n",
    "        for i in range(TIMES_COV):\n",
    "            fake_np0 = np.reshape(session.run(fake_data), [-1, OUTPUT_DIM])\n",
    "            fake_np1 = np.reshape(session.run(fake_data2), [-1, OUTPUT_DIM])\n",
    "            FGramm = np.matmul(fake_np0,np.transpose(fake_np1))\n",
    "            FR0 = np.reshape(np.sum(np.multiply(fake_np0,fake_np0), axis=1), [BATCH_SIZE,1])\n",
    "            FR1 = np.reshape(np.sum(np.multiply(fake_np1,fake_np1), axis=1), [BATCH_SIZE,1])\n",
    "            M_0 = np.exp(GAMMA*(-np.matmul(FR0, ft2) - np.matmul(ft1, np.transpose(FR1)) + 2*FGramm))\n",
    "            M = M + np.matmul(np.matmul(np.transpose(fake_np0), M_0), fake_np1)\n",
    "        svd = TruncatedSVD(n_components=Dimension, n_iter=7)\n",
    "        svd.fit(M)\n",
    "        U = svd.components_\n",
    "        P_retro = np.transpose(U)\n",
    "        print(svd.singular_values_)\n",
    "        EV = svd.explained_variance_.sum()\n",
    "        EVR = svd.explained_variance_ratio_.sum()\n",
    "        Projection = np.matmul(P_retro, np.transpose(P_retro))\n",
    "        DIFF = Projection - P_correct\n",
    "        print(\"Explained variance: %f and explained variance ratio: %f\" % (EV, EVR))       \n",
    "        dist = np.sqrt(np.sum(DIFF*DIFF))\n",
    "        lib.plot.plot('Distance till correct', dist)\n",
    "        L12 = np.sum(np.sqrt(np.sum(np.square(images-np.matmul(images, Projection)), axis=1)))\n",
    "        print(\"L12 distance %0.4f\\n\" %L12)\n",
    "        save_path = saver.save(session, \"/tmp/model.ckpt\")\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
