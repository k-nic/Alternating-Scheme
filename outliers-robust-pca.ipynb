{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rust/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tflib as lib\n",
    "import tflib.ops.linear\n",
    "import tflib.ops.conv2d\n",
    "import tflib.ops.batchnorm\n",
    "import tflib.ops.deconv2d\n",
    "import tflib.save_images\n",
    "import tflib.plot\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from keras_radam.training import RAdamOptimizer\n",
    "\n",
    "\n",
    "LAMBDA = 1000.0 # Gradient penalty lambda hyperparameter --- it regulates how strongly we enforce lipschitzness\n",
    "LAMBDA_2 = 5.0 # Constant for consistency term as in https://openreview.net/pdf?id=SJx9GQb0-\n",
    "CRITIC_ITERS = 5 # How many critic iterations per generator iteration\n",
    "BATCH_SIZE = 40 # Batch size\n",
    "ITERS = 3000 # How many generator iterations to train for\n",
    "num_epochs = 100 # number of iterations of alternating scheme\n",
    "OUTPUT_DIM = 400 # output dimension\n",
    "Lambda = 100.0 # the main lambda --- constant in front of the second term that enforces low-dimensionality of support \n",
    "Dimension = 10 # Hidden dimensionality of data --- rank of matrix L\n",
    "GAMMA = 1/OUTPUT_DIM # parameter of gaussian kernel M(x,y) = exp(-GAMMA*(x-y)^2)\n",
    "delta = 0.1 # regulates percentage of outliers\n",
    "MODE = 'neutral' # case I - neutral, case II-something else\n",
    "\n",
    "SIZE_COV = 10000*OUTPUT_DIM # how many times we sample fake and real to estimate matrix M_Ñ„ from step (**)\n",
    "TIMES_COV = int(SIZE_COV/BATCH_SIZE)\n",
    "SIZE_COV = TIMES_COV*BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 400)\n",
      "Frobenius distance from SVD till correct 0.0037\n",
      "L12 distance till correct 780.9285\n"
     ]
    }
   ],
   "source": [
    "# here I generate matrix X\n",
    "num = 400\n",
    "A = np.random.normal(loc=0.0, scale=1.0, size=(int(num*(1-delta)), Dimension))\n",
    "B = np.random.normal(loc=0.0, scale=1.0, size=(Dimension, OUTPUT_DIM))\n",
    "if MODE == 'neutral':\n",
    "    C = np.random.normal(loc=0.0, scale=1.0, size=(int(num*delta), OUTPUT_DIM))\n",
    "else:\n",
    "    C = np.tile(np.random.normal(loc=0.0, scale=1.0, size=(1, OUTPUT_DIM)), (int(num*delta), 1))\n",
    "images = np.concatenate((np.matmul(A, B), C), axis = 0)\n",
    "print(images.shape)\n",
    "u, s, vh = np.linalg.svd(np.matmul(A, B), full_matrices=True)\n",
    "D = vh[0:Dimension]\n",
    "P_correct = np.matmul(np.transpose(D), D)\n",
    "\n",
    "u, s, vh = np.linalg.svd(images, full_matrices=True)\n",
    "D = vh[0:Dimension]\n",
    "P_svd = np.matmul(np.transpose(D), D)\n",
    "DIFF = P_svd - P_correct\n",
    "dist = np.sqrt(np.sum(DIFF*DIFF))\n",
    "print(\"Frobenius distance from SVD till correct %0.4f\" %dist)\n",
    "\n",
    "L1 = np.sum(np.sqrt(np.sum(np.square(images-np.matmul(images, P_correct)), axis=1)))\n",
    "print(\"L12 distance till correct %0.4f\" %L1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-8974564de364>:38: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-3-8974564de364>:69: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/rust/anaconda3/lib/python3.7/site-packages/keras_radam/training.py:171: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "iter 99\ttrain disc cost\t7531.662109375\ttime\t0.050558247566223145\tdev disc cost\t9576.8603515625\tdev total cost\t-52.50078201293945\tdev leakage cost\t8562.404296875\n",
      "iter 199\ttrain disc cost\t5238.3291015625\ttime\t0.035777814388275146\n",
      "iter 299\ttrain disc cost\t4200.92138671875\ttime\t0.03605682373046875\n",
      "iter 399\ttrain disc cost\t3603.0244140625\ttime\t0.03627554416656494\n",
      "iter 499\ttrain disc cost\t3204.654296875\ttime\t0.03558861494064331\tdev disc cost\t3038.54931640625\tdev total cost\t-40.781009674072266\tdev leakage cost\t2933.021484375\n",
      "iter 599\ttrain disc cost\t2866.009765625\ttime\t0.034954416751861575\n",
      "iter 699\ttrain disc cost\t2614.773193359375\ttime\t0.035957927703857424\n",
      "iter 799\ttrain disc cost\t2385.8662109375\ttime\t0.03636386632919311\n",
      "iter 899\ttrain disc cost\t2171.916259765625\ttime\t0.03557912826538086\n",
      "iter 999\ttrain disc cost\t1989.6873779296875\ttime\t0.03629093647003174\tdev disc cost\t1933.7750244140625\tdev total cost\t-36.86981201171875\tdev leakage cost\t1843.6478271484375\n",
      "iter 1099\ttrain disc cost\t1813.693603515625\ttime\t0.03698309659957886\n",
      "iter 1199\ttrain disc cost\t1639.587158203125\ttime\t0.03625946044921875\n",
      "iter 1299\ttrain disc cost\t1478.4287109375\ttime\t0.035727643966674806\n",
      "iter 1399\ttrain disc cost\t1290.84033203125\ttime\t0.03555217742919922\n",
      "iter 1499\ttrain disc cost\t1051.9410400390625\ttime\t0.03525970220565796\tdev disc cost\t901.3180541992188\tdev total cost\t-27.04323387145996\tdev leakage cost\t821.8517456054688\n",
      "iter 1599\ttrain disc cost\t518.0147705078125\ttime\t0.035397000312805176\n",
      "iter 1699\ttrain disc cost\t62.618465423583984\ttime\t0.035771796703338625\n",
      "iter 1799\ttrain disc cost\t13.281061172485352\ttime\t0.035425329208374025\n",
      "iter 1899\ttrain disc cost\t4.795187950134277\ttime\t0.03658204793930054\n",
      "iter 1999\ttrain disc cost\t-1.6131880283355713\ttime\t0.0362893533706665\tdev disc cost\t-4.118704795837402\tdev total cost\t9.455129623413086\tdev leakage cost\t0.8408337831497192\n",
      "iter 2099\ttrain disc cost\t-1.862424612045288\ttime\t0.03511989593505859\n",
      "iter 2199\ttrain disc cost\t-9.455286026000977\ttime\t0.0350098991394043\n",
      "iter 2299\ttrain disc cost\t-13.235934257507324\ttime\t0.03636411190032959\n",
      "iter 2399\ttrain disc cost\t-17.388370513916016\ttime\t0.03879103660583496\n",
      "iter 2499\ttrain disc cost\t-20.422441482543945\ttime\t0.03889455556869507\tdev disc cost\t-21.60858917236328\tdev total cost\t25.38729476928711\tdev leakage cost\t0.7818207144737244\n",
      "iter 2599\ttrain disc cost\t-23.037839889526367\ttime\t0.03630638360977173\n",
      "iter 2699\ttrain disc cost\t-24.885704040527344\ttime\t0.0379298734664917\n",
      "iter 2799\ttrain disc cost\t-26.198720932006836\ttime\t0.038254437446594236\n",
      "iter 2899\ttrain disc cost\t-27.996652603149414\ttime\t0.03731796503067017\n",
      "iter 2999\ttrain disc cost\t-29.43362808227539\ttime\t0.03870964050292969\tdev disc cost\t-29.845911026000977\tdev total cost\t34.17339324951172\tdev leakage cost\t0.6557009220123291\n",
      "[1.01855664e-07 9.16091701e-08 8.86020289e-08 7.94525332e-08\n",
      " 7.62086297e-08 7.12306833e-08 6.94841471e-08 6.64948181e-08\n",
      " 6.27143602e-08 6.11877908e-08]\n",
      "Explained variance: 0.000000 and explained variance ratio: 0.458964\n",
      "L12 distance 23435.9252\n",
      "\n",
      "WARNING:tensorflow:From /home/rust/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "WARNING:tensorflow:From <ipython-input-3-8974564de364>:180: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "iter 3099\tDistance till correct\t4.4088325033996325\ttrain disc cost\t-29.271255493164062\ttime\t0.0484087061882019\tdev disc cost\t-19.59186553955078\tdev total cost\t30.69805908203125\tdev leakage cost\t3.2961678504943848\n",
      "iter 3199\ttrain disc cost\t-31.408042907714844\ttime\t0.036135275363922116\n",
      "iter 3299\ttrain disc cost\t-31.91040802001953\ttime\t0.03770216941833496\n",
      "iter 3399\ttrain disc cost\t-33.06659698486328\ttime\t0.036215620040893556\n",
      "iter 3499\ttrain disc cost\t-33.45871353149414\ttime\t0.03708418369293213\tdev disc cost\t-34.47513961791992\tdev total cost\t39.1368522644043\tdev leakage cost\t1.2776877880096436\n",
      "iter 3599\ttrain disc cost\t-34.480133056640625\ttime\t0.03600986480712891\n",
      "iter 3699\ttrain disc cost\t-35.247833251953125\ttime\t0.035817177295684816\n",
      "iter 3799\ttrain disc cost\t-36.0057258605957\ttime\t0.036709465980529786\n",
      "iter 3899\ttrain disc cost\t-36.91618728637695\ttime\t0.03651246547698975\n",
      "iter 3999\ttrain disc cost\t-37.03180694580078\ttime\t0.036394472122192385\tdev disc cost\t-37.597251892089844\tdev total cost\t42.26409149169922\tdev leakage cost\t1.1214957237243652\n",
      "iter 4099\ttrain disc cost\t-37.802467346191406\ttime\t0.03649967908859253\n",
      "iter 4199\ttrain disc cost\t-38.14482498168945\ttime\t0.037336151599884036\n",
      "iter 4299\ttrain disc cost\t-38.562381744384766\ttime\t0.03707347869873047\n",
      "iter 4399\ttrain disc cost\t-39.43034362792969\ttime\t0.038360025882720944\n",
      "iter 4499\ttrain disc cost\t-39.12567901611328\ttime\t0.037746536731719973\tdev disc cost\t-39.49274444580078\tdev total cost\t43.242828369140625\tdev leakage cost\t0.6874085664749146\n",
      "iter 4599\ttrain disc cost\t-40.138877868652344\ttime\t0.03679063320159912\n",
      "iter 4699\ttrain disc cost\t-40.699214935302734\ttime\t0.037041425704956055\n",
      "iter 4799\ttrain disc cost\t-40.26387405395508\ttime\t0.03607809066772461\n",
      "iter 4899\ttrain disc cost\t-40.940860748291016\ttime\t0.03567052602767944\n",
      "iter 4999\ttrain disc cost\t-41.225608825683594\ttime\t0.0356163763999939\tdev disc cost\t-41.7075309753418\tdev total cost\t46.62683868408203\tdev leakage cost\t2.01926326751709\n",
      "iter 5099\ttrain disc cost\t-41.20354080200195\ttime\t0.03599971055984497\n",
      "iter 5199\ttrain disc cost\t-41.070369720458984\ttime\t0.036408164501190186\n",
      "iter 5299\ttrain disc cost\t-41.239986419677734\ttime\t0.03586286544799805\n",
      "iter 5399\ttrain disc cost\t-41.19822692871094\ttime\t0.03631823778152466\n",
      "iter 5499\ttrain disc cost\t-41.64640808105469\ttime\t0.03555202722549439\tdev disc cost\t-40.582828521728516\tdev total cost\t46.42948532104492\tdev leakage cost\t2.6614394187927246\n",
      "iter 5599\ttrain disc cost\t-41.59294128417969\ttime\t0.03633563995361328\n",
      "iter 5699\ttrain disc cost\t-41.621280670166016\ttime\t0.036111741065979\n",
      "iter 5799\ttrain disc cost\t-41.71788024902344\ttime\t0.03573022127151489\n",
      "iter 5899\ttrain disc cost\t-41.40380859375\ttime\t0.03633244514465332\n",
      "iter 5999\ttrain disc cost\t-41.696571350097656\ttime\t0.03599337577819824\tdev disc cost\t-42.04365539550781\tdev total cost\t45.648353576660156\tdev leakage cost\t1.2414577007293701\n"
     ]
    }
   ],
   "source": [
    "def get_vars_from_scope (scope = None):\n",
    "    ret = []\n",
    "    for var in tf.global_variables():\n",
    "        if re.search(scope, var.name):\n",
    "            ret.append(var)\n",
    "    return ret\n",
    "\n",
    "init_scale = 0.05\n",
    "highway_size = OUTPUT_DIM\n",
    "def highway_layer(highway_inputs):\n",
    "    transf_weights = tf.get_variable('transf_weights', \n",
    "        [highway_size, highway_size],\n",
    "        initializer=tf.zeros_initializer(),\n",
    "        dtype=tf.float32)\n",
    "    transf_biases = tf.get_variable('transf_biases', [highway_size],\n",
    "        initializer=tf.zeros_initializer(),\n",
    "        dtype=tf.float32)\n",
    "    highw_weights = tf.get_variable('highw_weights', \n",
    "        [highway_size, highway_size],\n",
    "        initializer=tf.initializers.identity(),\n",
    "        dtype=tf.float32)\n",
    "    highw_biases = tf.get_variable('highw_biases', [highway_size],\n",
    "        initializer=tf.zeros_initializer(),\n",
    "        dtype=tf.float32)\n",
    "    transf_gate = tf.nn.sigmoid(tf.matmul(highway_inputs, transf_weights)         + transf_biases)\n",
    "    highw_output = tf.multiply(transf_gate, \n",
    "        tf.nn.relu(tf.matmul(highway_inputs, highw_weights) + highw_biases)) \\\n",
    "        + tf.multiply(tf.ones([highway_size], dtype=tf.float32) - transf_gate, \n",
    "        highway_inputs)\n",
    "    return highw_output\n",
    "\n",
    "def Generator(n_samples, noise=None):\n",
    "    if noise is None:\n",
    "        noise = tf.random.uniform(shape=[n_samples], minval=0, maxval=OUTPUT_DIM, dtype=tf.dtypes.int32)\n",
    "        output = tf.one_hot(noise, OUTPUT_DIM, axis=-1)\n",
    "        noise1 = tf.random_normal([n_samples, OUTPUT_DIM])\n",
    "    with tf.variable_scope('Generator', reuse=tf.AUTO_REUSE) as scope:\n",
    "        output = tf.layers.dense(output, OUTPUT_DIM, activation=None)\n",
    "        output1 = tf.layers.dense(noise1, OUTPUT_DIM, activation=tf.nn.tanh)\n",
    "        output1 = 10*tf.layers.dense(output1, OUTPUT_DIM, activation=tf.nn.tanh)\n",
    "    return tf.reshape(output+output1, [-1, OUTPUT_DIM])\n",
    "\n",
    "def Generator_retro(n_samples, noise=None):\n",
    "    if noise is None:\n",
    "        noise = tf.random.uniform(shape=[n_samples], minval=0, maxval=OUTPUT_DIM, dtype=tf.dtypes.int32)\n",
    "        output = tf.one_hot(noise, OUTPUT_DIM, axis=-1)\n",
    "        noise1 = tf.random_normal([n_samples, OUTPUT_DIM])\n",
    "    with tf.variable_scope('Retro', reuse=tf.AUTO_REUSE) as scope:\n",
    "        output = tf.layers.dense(output, OUTPUT_DIM, activation=None)\n",
    "        output1 = tf.layers.dense(noise1, OUTPUT_DIM, activation=tf.nn.tanh)\n",
    "        output1 = 10*tf.layers.dense(output1, OUTPUT_DIM, activation=tf.nn.tanh)\n",
    "    return tf.reshape(output+output1, [-1, OUTPUT_DIM])\n",
    "\n",
    "def Copy_generators():\n",
    "    i=0\n",
    "    for h in gen_params:\n",
    "        tf.assign(gen_params_retro[i], h)\n",
    "        i=i+1\n",
    "\n",
    "\n",
    "def Discriminator(inputs):\n",
    "    output = tf.reshape(inputs, [-1, OUTPUT_DIM])\n",
    "    \n",
    "    with tf.variable_scope('Discriminator', reuse=tf.AUTO_REUSE) as scope:\n",
    "        with tf.variable_scope('highway1', reuse=tf.AUTO_REUSE):\n",
    "            output = highway_layer(output)\n",
    "        with tf.variable_scope('highway2', reuse=tf.AUTO_REUSE):\n",
    "            output = highway_layer(output)\n",
    "        output = tf.nn.dropout(output, keep_prob=0.50)\n",
    "        with tf.variable_scope('highway3', reuse=tf.AUTO_REUSE):\n",
    "            output = highway_layer(output)\n",
    "        with tf.variable_scope('highway4', reuse=tf.AUTO_REUSE):\n",
    "            output = highway_layer(output)\n",
    "        output = tf.nn.dropout(output, keep_prob=0.50)\n",
    "        with tf.variable_scope('highway5', reuse=tf.AUTO_REUSE):\n",
    "            output = highway_layer(output)\n",
    "        with tf.variable_scope('highway6', reuse=tf.AUTO_REUSE):\n",
    "            output = highway_layer(output)\n",
    "        output = tf.math.sqrt(tf.reduce_sum(tf.square(output), axis=1)+0.0001)\n",
    "    return tf.reshape(output, [-1])\n",
    "\n",
    "P = tf.placeholder(tf.float32, shape=[OUTPUT_DIM, Dimension])\n",
    "real_data = tf.placeholder(tf.float32, shape=[BATCH_SIZE, OUTPUT_DIM])\n",
    "fake_data = Generator(BATCH_SIZE)\n",
    "fake_data2 = Generator(BATCH_SIZE)\n",
    "disc_real = Discriminator(real_data)\n",
    "disc_real_= Discriminator(real_data)\n",
    "disc_fake = Discriminator(fake_data)\n",
    "\n",
    "gen_params = get_vars_from_scope('Generator')\n",
    "disc_params = get_vars_from_scope('Discriminator')\n",
    "\n",
    "\n",
    "fake_data_retro = Generator_retro(BATCH_SIZE)\n",
    "gen_params_retro = get_vars_from_scope('Retro')\n",
    "\n",
    "gen_cost = -tf.reduce_mean(disc_fake)\n",
    "disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)\n",
    "\n",
    "#Consistency term from https://openreview.net/pdf?id=SJx9GQb0-\n",
    "CT = LAMBDA_2*tf.square(disc_real-disc_real_)\n",
    "CT_ = tf.maximum(CT-0.0,0.0*CT)\n",
    "disc_cost += tf.reduce_mean(CT_)\n",
    "\n",
    "# Gradient penalty\n",
    "alpha = tf.random_uniform(shape=[BATCH_SIZE,1], minval=0.,maxval=1.)\n",
    "differences = fake_data - real_data\n",
    "interpolates = real_data + (alpha*differences)\n",
    "gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]\n",
    "\n",
    "slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "gradient_penalty = tf.reduce_mean(tf.clip_by_value(slopes - 1.0, 0, np.infty)**2)\n",
    "\n",
    "disc_cost += LAMBDA*gradient_penalty\n",
    "\n",
    "# I needed to know how strongly \"leakage\" can blow-up\n",
    "leakage = LAMBDA*gradient_penalty \n",
    "\n",
    "Q = tf.reshape(fake_data, [BATCH_SIZE,-1])\n",
    "Q2 = tf.reshape(fake_data2, [BATCH_SIZE,-1])\n",
    "Gramm = tf.matmul(Q,tf.transpose(Q2))\n",
    "R = tf.reshape(tf.reduce_sum(tf.multiply(Q,Q), axis=1), [BATCH_SIZE,1])\n",
    "R2 = tf.reshape(tf.reduce_sum(tf.multiply(Q2,Q2), axis=1), [BATCH_SIZE,1])\n",
    "t1 = tf.ones([BATCH_SIZE, 1])\n",
    "t2 = tf.ones([1, BATCH_SIZE])\n",
    "S = tf.multiply(tf.exp(GAMMA*(-tf.matmul(R, t2) - tf.matmul(t1, tf.transpose(R2)) + 2*Gramm)), Gramm)\n",
    "\n",
    "Q_retro = tf.reshape(fake_data_retro, [BATCH_SIZE,-1])\n",
    "R_retro = tf.reshape(tf.reduce_sum(tf.multiply(Q_retro,Q_retro), axis=1), [BATCH_SIZE,1])\n",
    "Gramm2 = tf.matmul(Q,tf.transpose(Q_retro))\n",
    "Q3 = tf.matmul(tf.matmul(Q_retro, P), tf.transpose(P))\n",
    "Gramm3 = tf.matmul(Q,tf.transpose(Q3))\n",
    "S2 = tf.multiply(tf.exp(GAMMA*(-tf.matmul(R, t2) - tf.matmul(t1, tf.transpose(R_retro)) + 2*Gramm2)), Gramm3)\n",
    "\n",
    "gen_cost += Lambda*tf.reduce_mean(S) - 2*Lambda*tf.reduce_mean(S2)\n",
    "\n",
    "total = tf.reduce_mean(disc_real) - tf.reduce_mean(disc_fake) + Lambda*tf.reduce_mean(S) - 2*Lambda*tf.reduce_mean(S2)\n",
    "\n",
    "\n",
    "# Radam converges faster than Adam, so I use it\n",
    "optimizer_gen = RAdamOptimizer(learning_rate=1e-5, beta1=0.5, beta2=0.9)\n",
    "optimizer_disc = RAdamOptimizer(learning_rate=1e-5, beta1=0.5, beta2=0.9)\n",
    "gen_train_op = optimizer_gen.minimize(gen_cost, var_list=gen_params)\n",
    "disc_train_op = optimizer_disc.minimize(disc_cost, var_list=disc_params)\n",
    "\n",
    "\n",
    "# Also, weights of discriminator are clipped as in original paper of Arjovsky et al, \n",
    "# though this is not needed in fact \n",
    "clip_ops = []\n",
    "for var in disc_params:\n",
    "    clip_bounds = [-10.0, 10.0]\n",
    "    clip_ops.append(\n",
    "        tf.assign(\n",
    "            var, \n",
    "            tf.clip_by_value(var, clip_bounds[0], clip_bounds[1])\n",
    "        )\n",
    "    )\n",
    "clip_disc_weights = tf.group(*clip_ops)\n",
    "\n",
    "\n",
    "num = images.shape[0]\n",
    "images = np.reshape(images, (num, -1))\n",
    "data = np.random.permutation(images)\n",
    "\n",
    "# Train loop\n",
    "P_retro = np.zeros((OUTPUT_DIM,Dimension))\n",
    "current = 0\n",
    "c_size = int(num/BATCH_SIZE)\n",
    "ft1 = np.ones([BATCH_SIZE, 1])\n",
    "ft2 = np.ones([1, BATCH_SIZE])\n",
    "needed_vars = gen_params + disc_params\n",
    "saver = tf.train.Saver(var_list=needed_vars)\n",
    "for epoch in range(num_epochs):\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.initialize_all_variables())\n",
    "        if(epoch>0):\n",
    "            saver.restore(session, \"/tmp/model.ckpt\")\n",
    "            i=0\n",
    "            for h in gen_params:\n",
    "                gen_params_retro[i].load(session.run(h), session)\n",
    "                i=i+1\n",
    "        for iteration in range(ITERS):\n",
    "            start_time = time.time()\n",
    "            # Train generator\n",
    "            if iteration > 0:\n",
    "                start_index = current % c_size\n",
    "                start_index = start_index*BATCH_SIZE\n",
    "                _data = data[start_index:start_index+BATCH_SIZE]\n",
    "                _ = session.run(gen_train_op, feed_dict={real_data: _data, P: P_retro})\n",
    "                current = current+1\n",
    "            # Train critic\n",
    "            disc_iters = CRITIC_ITERS\n",
    "            for i in range(disc_iters):\n",
    "                start_index = current % c_size\n",
    "                start_index = start_index*BATCH_SIZE\n",
    "                _data = data[start_index:start_index+BATCH_SIZE]\n",
    "                _disc_cost, _ = session.run([disc_cost, disc_train_op], feed_dict={real_data: _data, P: P_retro})\n",
    "                _ = session.run(clip_disc_weights)\n",
    "                current = current+1\n",
    "\n",
    "            lib.plot.plot('train disc cost', _disc_cost)\n",
    "            lib.plot.plot('time', time.time() - start_time)\n",
    "\n",
    "            # Calculate dev loss and generate samples every 500 iters\n",
    "            if (epoch*ITERS+iteration) % 500 == 499 or iteration == 0:\n",
    "                dev_disc_costs = []\n",
    "                dev_total = []\n",
    "                dev_leakage = []\n",
    "                for image in range(c_size):\n",
    "                    start_index = image*BATCH_SIZE\n",
    "                    _dev_disc_cost, _total, _leakage = session.run([disc_cost, total, leakage], feed_dict={real_data: \n",
    "                                                    data[start_index:start_index+BATCH_SIZE], P: P_retro}) \n",
    "                    dev_disc_costs.append(_dev_disc_cost)\n",
    "                    dev_total.append(_total)\n",
    "                    dev_leakage.append(_leakage)\n",
    "                lib.plot.plot('dev disc cost', np.mean(dev_disc_costs))\n",
    "                lib.plot.plot('dev total cost', np.mean(dev_total))\n",
    "                lib.plot.plot('dev leakage cost', np.mean(dev_leakage))\n",
    "\n",
    "            # Save logs every 100 iters\n",
    "            if (iteration % 100 == 99): #(iteration < 2) or \n",
    "                lib.plot.flush()\n",
    "\n",
    "            lib.plot.tick()\n",
    "        M = np.zeros((OUTPUT_DIM,OUTPUT_DIM))\n",
    "        for i in range(TIMES_COV):\n",
    "            fake_np0 = np.reshape(session.run(fake_data), [-1, OUTPUT_DIM])\n",
    "            fake_np1 = np.reshape(session.run(fake_data2), [-1, OUTPUT_DIM])\n",
    "            FGramm = np.matmul(fake_np0,np.transpose(fake_np1))\n",
    "            FR0 = np.reshape(np.sum(np.multiply(fake_np0,fake_np0), axis=1), [BATCH_SIZE,1])\n",
    "            FR1 = np.reshape(np.sum(np.multiply(fake_np1,fake_np1), axis=1), [BATCH_SIZE,1])\n",
    "            M_0 = np.exp(GAMMA*(-np.matmul(FR0, ft2) - np.matmul(ft1, np.transpose(FR1)) + 2*FGramm))\n",
    "            M = M + np.matmul(np.matmul(np.transpose(fake_np0), M_0), fake_np1)\n",
    "        svd = TruncatedSVD(n_components=Dimension, n_iter=7)\n",
    "        svd.fit(M)\n",
    "        U = svd.components_\n",
    "        P_retro = np.transpose(U)\n",
    "        print(svd.singular_values_)\n",
    "        EV = svd.explained_variance_.sum()\n",
    "        EVR = svd.explained_variance_ratio_.sum()\n",
    "        Projection = np.matmul(P_retro, np.transpose(P_retro))\n",
    "        DIFF = Projection - P_correct\n",
    "        print(\"Explained variance: %f and explained variance ratio: %f\" % (EV, EVR))       \n",
    "        dist = np.sqrt(np.sum(DIFF*DIFF))\n",
    "        lib.plot.plot('Distance till correct', dist)\n",
    "        L12 = np.sum(np.sqrt(np.sum(np.square(images-np.matmul(images, Projection)), axis=1)))\n",
    "        print(\"L12 distance %0.4f\\n\" %L12)\n",
    "        save_path = saver.save(session, \"/tmp/model.ckpt\")\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
