{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Exploring the SIR Data Reduction Method for Visualization\n",
    "In his 1991 paper, Prof. Ker-Chau Li (UCLA) introduced a fascinating method for supervised dimensionality reduction called SIR (Sliced Inverse Regression) assuming the following model:\n",
    "\n",
    "$$Y  = f(\\beta_1 \\mathbf{X}, \\dots, \\beta_K \\mathbf{X},\\epsilon ) $$\n",
    "\n",
    "Where $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$, and $f$ can be any function on $\\mathbb{R}^{K+1}.$ Without delving into theory the method works by considering the inverse regression curve $E(\\mathbf{X}|Y)$, and estimating $E(\\mathbf{X}|Y)$ via slicing. Under some mild assumptions (and assuming $\\mathbf{X}$ has been standardized), the inverse regression curve is contained in the subspace spanned by the $\\beta_1,\\dots, \\beta_K$. The method uses a principal components analysis on the covariance matrix of the inverse regression curve to estimate its orientation. \n",
    "\n",
    "Implemented the basic SIR method in python with three methods:\n",
    "- fit\n",
    "- transform\n",
    "- fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from SIR import SIR\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import load_diabetes\n",
    "import matplotlib.pyplot as plt2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline \n",
    "\n",
    "from SufficientDR import sdr\n",
    "from sliced import SlicedAverageVarianceEstimation\n",
    "from sliced import SlicedInverseRegression\n",
    "\n",
    "dim_k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston(return_X_y=False)\n",
    "print(np.size(np.array(boston['target'])))\n",
    "data = []\n",
    "target = []\n",
    "rr = np.random.permutation(506)\n",
    "for x in rr:\n",
    "    data.append(boston['data'][x])\n",
    "    target.append(boston['target'][x])\n",
    "# training data\n",
    "X1 = np.array(data)\n",
    "Y1 = np.array(target)\n",
    "print(X1.shape)\n",
    "print(Y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast = load_breast_cancer(return_X_y=False)\n",
    "data = []\n",
    "target = []\n",
    "rr = np.random.permutation(569)\n",
    "for x in rr:\n",
    "    data.append(breast['data'][x])\n",
    "    target.append(breast['target'][x])\n",
    "# training data\n",
    "X2 = np.array(data)\n",
    "Y2 = np.array(target)\n",
    "print(X2.shape)\n",
    "print(Y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes(return_X_y=False)\n",
    "data = []\n",
    "target = []\n",
    "rr = np.random.permutation(442)\n",
    "for x in rr:\n",
    "    data.append(diabetes['data'][x])\n",
    "    target.append(diabetes['target'][x])\n",
    "# training data\n",
    "X3 = np.array(data)\n",
    "Y3 = np.array(target)\n",
    "print(X3.shape)\n",
    "print(Y3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart = pd.read_csv('heart.csv')\n",
    "X = heart.iloc[:,:-1].values\n",
    "y = heart.iloc[:,13].values\n",
    "print(len(y))\n",
    "data = []\n",
    "target = []\n",
    "rr = np.random.permutation(303)\n",
    "for x in rr:\n",
    "    data.append(X[x])\n",
    "    target.append(y[x])\n",
    "# training data\n",
    "X4 = np.array(data)\n",
    "Y4 = np.array(target)\n",
    "print(X4.shape)\n",
    "print(Y4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "breast = load_wine(return_X_y=False)\n",
    "data = []\n",
    "target = []\n",
    "rr = np.random.permutation(178)\n",
    "for x in rr:\n",
    "    data.append(breast['data'][x])\n",
    "    target.append(breast['target'][x])\n",
    "# training data\n",
    "X5 = np.array(data)\n",
    "Y5 = np.array(target)\n",
    "print(X5.shape)\n",
    "print(Y5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ionosphere = pd.read_csv('ionosphere.data', header=None)\n",
    "X = ionosphere.iloc[:,:-1].values\n",
    "y = ionosphere.iloc[:,34].values\n",
    "print(len(y))\n",
    "data = []\n",
    "target = []\n",
    "rr = np.random.permutation(351)\n",
    "for x in rr:\n",
    "    data.append(X[x].astype(np.float)+0.000001*np.random.rand(34))\n",
    "    if y[x] == 'g':\n",
    "        target.append(1)\n",
    "    else:\n",
    "        target.append(0)\n",
    "# training data\n",
    "X6 = np.array(data)\n",
    "Y6 = np.array(target)\n",
    "print(X6.shape)\n",
    "print(Y6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(pnt1, pnt2):\n",
    "    return sum((pnt1 - pnt2) ** 2)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_majority(labels):\n",
    "    counter = defaultdict(int)\n",
    "    for label in labels:\n",
    "        counter[label] += 1\n",
    "\n",
    "    majority_count = max(counter.values())\n",
    "    for key, value in counter.items():\n",
    "        if value == majority_count:\n",
    "            return key\n",
    "\n",
    "def new_predict(k, train_pnts, train_labels, test_pnts):\n",
    "    distances = [(euclidean_distance(test_pnts, pnt), label)\n",
    "                    for (pnt, label) in zip(train_pnts, train_labels)]\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    return find_majority(k_labels)\n",
    "\n",
    "def new_predict_regr(k, train_pnts, train_labels, test_pnts):\n",
    "    distances = [(euclidean_distance(test_pnts, pnt), label)\n",
    "                    for (pnt, label) in zip(train_pnts, train_labels)]\n",
    "    compare = lambda distance: distance[0]\n",
    "    by_distances = sorted(distances, key=compare)\n",
    "    k_labels = [label for (_, label) in by_distances[:k]]\n",
    "    return np.mean(k_labels)\n",
    "\n",
    "def score_knn(reduced_train_x, reduced_train_y, reduced_test_x, reduced_test_y):\n",
    "    i = 0\n",
    "    total_correct = 0\n",
    "    for test_image in reduced_test_x:\n",
    "        pred = new_predict(10, reduced_train_x, reduced_train_y, test_image)\n",
    "        if pred == reduced_test_y[i]:\n",
    "            total_correct += 1\n",
    "        i += 1\n",
    "    score = (total_correct / i) * 100\n",
    "    return score\n",
    "\n",
    "def score_knn_regr(reduced_train_x, reduced_train_y, reduced_test_x, reduced_test_y):\n",
    "    test_y_prediction = []\n",
    "    for test_image in reduced_test_x:\n",
    "        pred = new_predict_regr(10, reduced_train_x, train_y, test_image)\n",
    "        test_y_prediction.append(pred)\n",
    "    score = 100*(1.0-np.mean((np.array(test_y_prediction)- reduced_test_y)**2)/np.var(reduced_test_y))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_sir_save_as(train_x, train_y, test_x, test_y, batch_size, if_regr = 0):\n",
    "    if if_regr==1:\n",
    "        score_knn_function = score_knn_regr\n",
    "    else:\n",
    "        score_knn_function = score_knn\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "    train_x = scaler.transform(train_x)\n",
    "    test_x = scaler.transform(test_x)\n",
    "\n",
    "    pca = PCA(n_components =dim_k)\n",
    "    pca.fit(train_x)\n",
    "    pca_train_x = pca.transform(train_x)\n",
    "    pca_test_x = pca.transform(test_x)\n",
    "    score_pca = score_knn_function(pca_train_x, train_y, pca_test_x, test_y)\n",
    "    scores_pca.append(score_pca)\n",
    "    print('Accuracy of PCA-KNN:', str(round(score_pca, 2))+'%')\n",
    "\n",
    "    sir_1 = SlicedInverseRegression(n_directions=dim_k)\n",
    "    sir_1.fit(train_x,train_y)\n",
    "    sir_train_x = np.real(sir_1.transform(train_x))\n",
    "    sir_test_x = np.real(sir_1.transform(test_x))\n",
    "    score_sir = score_knn_function(sir_train_x, train_y, sir_test_x, test_y)\n",
    "    scores_sir.append(score_sir)\n",
    "    print('Accuracy of SIR-KNN:', str(round(score_sir, 2))+'%')\n",
    "\n",
    "    save_1 = SlicedAverageVarianceEstimation(n_directions=dim_k)\n",
    "    save_1.fit(train_x,train_y)\n",
    "    save_train_x = save_1.transform(train_x)\n",
    "    save_test_x = save_1.transform(test_x)\n",
    "    score_save = score_knn_function(save_train_x, train_y, save_test_x, test_y)\n",
    "    scores_save.append(score_save)\n",
    "    print('Accuracy of SAVE-KNN:', str(round(score_save, 2))+'%')\n",
    "    \n",
    "    if if_regr==1:\n",
    "        O = sdr(train_x, train_y, k=dim_k, Lambda = 10.0, number_of_neurons = 50, BATCH_SIZE=batch_size, num_epochs = 30, classify=False)\n",
    "    else:\n",
    "        O = sdr(train_x, train_y, k=dim_k, Lambda = 10.0, number_of_neurons = 50, BATCH_SIZE=batch_size, num_epochs = 30, classify=True)\n",
    "    as_train_x = np.matmul(train_x, O)\n",
    "    as_test_x = np.matmul(test_x, O)\n",
    "    score_as = score_knn_function(as_train_x, train_y, as_test_x, test_y)\n",
    "    scores_as.append(score_as)\n",
    "    print('Accuracy of AS-KNN:', str(round(score_as, 2))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Boston \n",
    "\n",
    "We have data $\\mathbf{X1}\\in \\mathbb{R}^{506x13}$ and:\n",
    "$$Y1 = f(X1 +\\epsilon)$$\n",
    "\n",
    "with $\\epsilon \\sim N(0,1)$ \n",
    "\n",
    "Using Boston data $(\\mathbf{X1},Y1)_{i=2}^{506}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores_pca = []\n",
    "scores_sir = []\n",
    "scores_save = []\n",
    "scores_as = []\n",
    "for part in range(10):\n",
    "    print(\"Current part is %d\\n\" % part)\n",
    "    train_x = np.concatenate((X1[0:(part*50)], X1[(part+1)*50:506]), axis=0)\n",
    "    train_y = np.concatenate((Y1[0:(part*50)], Y1[(part+1)*50:506]), axis=0)\n",
    "    test_x = X1[part*50:(part+1)*50]\n",
    "    test_y = Y1[part*50:(part+1)*50]\n",
    "    pca_sir_save_as(train_x, train_y, test_x, test_y, batch_size=38, if_regr=1)\n",
    "print (\"Average PCA score on test %.4f\\n\" %(np.mean(np.array(scores_pca))))\n",
    "print (\"Average SIR score on test %.4f\\n\" %(np.mean(np.array(scores_sir))))\n",
    "print (\"Average SAVE score on test %.4f\\n\" %(np.mean(np.array(scores_save))))\n",
    "print (\"Average AS score on test %.4f\\n\" %(np.mean(np.array(scores_as))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Breast cancer \n",
    "\n",
    "We have data $\\mathbf{X2}\\in \\mathbb{R}^{569x30}$ and:\n",
    "$$Y2 = f(X2 +\\epsilon)^2$$\n",
    "\n",
    "with $\\epsilon \\sim N(0,1)$ \n",
    "\n",
    "Generating the data $(\\mathbf{X2},Y2)_{i=2}^{569}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores_pca = []\n",
    "scores_sir = []\n",
    "scores_save = []\n",
    "scores_as = []\n",
    "for part in range(8):\n",
    "    print(\"Current part is %d\\n\" % part)\n",
    "    train_x = np.concatenate((X2[0:(part*65)], X2[(part+1)*65:569]), axis=0)\n",
    "    train_y = np.concatenate((Y2[0:(part*65)], Y2[(part+1)*65:569]), axis=0)\n",
    "    test_x = X2[part*65:(part+1)*65]\n",
    "    test_y = Y2[part*65:(part+1)*65]\n",
    "    pca_sir_save_as(train_x, train_y, test_x, test_y, batch_size=56, if_regr=0)\n",
    "print (\"Average PCA score on test %.4f\\n\" %(np.mean(np.array(scores_pca))))\n",
    "print (\"Average SIR score on test %.4f\\n\" %(np.mean(np.array(scores_sir))))\n",
    "print (\"Average SAVE score on test %.4f\\n\" %(np.mean(np.array(scores_save))))\n",
    "print (\"Average AS score on test %.4f\\n\" %(np.mean(np.array(scores_as))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Следующая ячейка с экспериментом k-NN: он не удался, не могу найти ошибку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Diabetes\n",
    "\n",
    "We have data $\\mathbf{X3}\\in \\mathbb{R}^{442x10}$ and:\n",
    "$$Y3 = f(X3 +\\epsilon)$$\n",
    "\n",
    "with $\\epsilon \\sim N(0,1)$ \n",
    "\n",
    "Generating the data $(\\mathbf{X},Y)_{i=2}^{442}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores_pca = []\n",
    "scores_sir = []\n",
    "scores_save = []\n",
    "scores_as = []\n",
    "for part in range(10):\n",
    "    print(\"Current part is %d\\n\" % part)\n",
    "    train_x = np.concatenate((X3[0:(part*37)], X3[(part+1)*37:442]), axis=0)\n",
    "    train_y = np.concatenate((Y3[0:(part*37)], Y3[(part+1)*37:442]), axis=0)\n",
    "    test_x = X3[part*37:(part+1)*37]\n",
    "    test_y = Y3[part*37:(part+1)*37]\n",
    "    pca_sir_save_as(train_x, train_y, test_x, test_y, batch_size=45, if_regr=1)\n",
    "print (\"Average PCA score on test %.4f\\n\" %(np.mean(np.array(scores_pca))))\n",
    "print (\"Average SIR score on test %.4f\\n\" %(np.mean(np.array(scores_sir))))\n",
    "print (\"Average SAVE score on test %.4f\\n\" %(np.mean(np.array(scores_save))))\n",
    "print (\"Average AS score on test %.4f\\n\" %(np.mean(np.array(scores_as))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Heart Disease \n",
    "\n",
    "We have data $\\mathbf{X4}\\in \\mathbb{R}^{303x13}$ and:\n",
    "$$Y4 = f(X4 +\\epsilon)$$\n",
    "\n",
    "with $\\epsilon \\sim N(0,1)$ \n",
    "\n",
    "Using Heart Disease data $(\\mathbf{X4},Y4)_{i=2}^{303}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores_pca = []\n",
    "scores_sir = []\n",
    "scores_save = []\n",
    "scores_as = []\n",
    "for part in range(9):\n",
    "    print(\"Current part is %d\\n\" % part)\n",
    "    train_x = np.concatenate((X4[0:(part*33)], X4[(part+1)*33:303]), axis=0)\n",
    "    train_y = np.concatenate((Y4[0:(part*33)], Y4[(part+1)*33:303]), axis=0)\n",
    "    test_x = X4[part*33:(part+1)*33]\n",
    "    test_y = Y4[part*33:(part+1)*33]\n",
    "    pca_sir_save_as(train_x, train_y, test_x, test_y, batch_size=30, if_regr=0)\n",
    "print (\"Average PCA score on test %.4f\\n\" %(np.mean(np.array(scores_pca))))\n",
    "print (\"Average SIR score on test %.4f\\n\" %(np.mean(np.array(scores_sir))))\n",
    "print (\"Average SAVE score on test %.4f\\n\" %(np.mean(np.array(scores_save))))\n",
    "print (\"Average AS score on test %.4f\\n\" %(np.mean(np.array(scores_as))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores_pca = []\n",
    "scores_sir = []\n",
    "scores_save = []\n",
    "scores_as = []\n",
    "for part in range(8):\n",
    "    print(\"Current part is %d\\n\" % part)\n",
    "    train_x = np.concatenate((X5[0:(part*22)], X5[(part+1)*22:178]), axis=0)\n",
    "    train_y = np.concatenate((Y5[0:(part*22)], Y5[(part+1)*22:178]), axis=0)\n",
    "    test_x = X5[part*22:(part+1)*22]\n",
    "    test_y = Y5[part*22:(part+1)*22]\n",
    "    pca_sir_save_as(train_x, train_y, test_x, test_y, batch_size=26, if_regr=1)\n",
    "print (\"Average PCA score on test %.4f\\n\" %(np.mean(np.array(scores_pca))))\n",
    "print (\"Average SIR score on test %.4f\\n\" %(np.mean(np.array(scores_sir))))\n",
    "print (\"Average SAVE score on test %.4f\\n\" %(np.mean(np.array(scores_save))))\n",
    "print (\"Average AS score on test %.4f\\n\" %(np.mean(np.array(scores_as))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pca = []\n",
    "scores_sir = []\n",
    "scores_save = []\n",
    "scores_as = []\n",
    "for part in range(10):\n",
    "    print(\"Current part is %d\\n\" % part)\n",
    "    train_x = np.concatenate((X6[0:(part*35)], X6[(part+1)*35:351]), axis=0)\n",
    "    train_y = np.concatenate((Y6[0:(part*35)], Y6[(part+1)*35:351]), axis=0)\n",
    "    test_x = X6[part*35:(part+1)*35]\n",
    "    test_y = Y6[part*35:(part+1)*35]\n",
    "    pca_sir_save_as(train_x, train_y, test_x, test_y, batch_size=79, if_regr=0)    \n",
    "print (\"Average PCA score on test %.4f\\n\" %(np.mean(np.array(scores_pca))))\n",
    "print (\"Average SIR score on test %.4f\\n\" %(np.mean(np.array(scores_sir))))\n",
    "print (\"Average SAVE score on test %.4f\\n\" %(np.mean(np.array(scores_save))))\n",
    "print (\"Average AS score on test %.4f\\n\" %(np.mean(np.array(scores_as))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
