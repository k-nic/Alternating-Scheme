{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "F1Y_8jYdjORy",
    "outputId": "36ff5c4b-0fc7-4bb1-e76f-f901264c8989"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "import scipy as scipy\n",
    "import scipy.interpolate as interpolate\n",
    "import math\n",
    "from random import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from scipy.stats import logistic\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt2\n",
    "\n",
    "import tflib as lib\n",
    "import tflib.ops.linear\n",
    "import tflib.ops.conv2d\n",
    "import tflib.ops.batchnorm\n",
    "import tflib.ops.deconv2d\n",
    "import tflib.save_images\n",
    "import tflib.plot\n",
    "\n",
    "\n",
    "Lambda = 10.0\n",
    "BATCH_SIZE = 56\n",
    "BATCH_SIZE_2 = 100\n",
    "number_of_neurons = 200\n",
    "dimensionality = 30\n",
    "num_epochs = 20\n",
    "num_iters = 60\n",
    "M = 3000\n",
    "k=2\n",
    "max_grad_norm = 1000\n",
    "gamma = 1.0\n",
    "epsilon = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5ExlL7s3o9S6",
    "outputId": "82a79e11-a546-4fa7-81a1-de76f5b0011d"
   },
   "outputs": [],
   "source": [
    "breast = load_breast_cancer(return_X_y=False)\n",
    "data = []\n",
    "target = []\n",
    "rr = np.random.permutation(569)\n",
    "for x in rr:\n",
    "    data.append(breast['data'][x])\n",
    "    target.append(breast['target'][x])\n",
    "# training data\n",
    "_train_x = np.array(data)\n",
    "_train_y = np.array(target)\n",
    "print(_train_x.shape)\n",
    "print(_train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvKNXjixjm-A"
   },
   "outputs": [],
   "source": [
    "new_lr = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "batch_size = tf.placeholder(tf.int32, shape=[], name=\"batch_size\")\n",
    " \n",
    "tf_data_x = tf.placeholder(tf.float32, shape=(None,dimensionality)) # узел на который будем подавать аргументы функции\n",
    "tf_data_y = tf.placeholder(tf.float32, shape=(None,1)) # узел на который будем подавать значения функции\n",
    "\n",
    "tf_data_w = tf.placeholder(tf.float32, shape=(number_of_neurons,1))\n",
    "tf_data_B = tf.placeholder(tf.float32, shape=(dimensionality, number_of_neurons))\n",
    "tf_data_biases = tf.placeholder(tf.float32, shape=(number_of_neurons))\n",
    "tf_data_P = tf.placeholder(tf.float32, shape=(dimensionality, dimensionality))\n",
    "\n",
    "tf_data_second = gamma*tf.random_normal([BATCH_SIZE_2, dimensionality])\n",
    "Lbd = tf.placeholder(tf.float32, shape=[], name=\"lambda\")\n",
    "\n",
    "w = tf.Variable(tf.random_normal([number_of_neurons,1], stddev=0.35), name=\"neuron_weights\")\n",
    "B = tf.Variable(tf.random_normal([dimensionality, number_of_neurons], stddev=0.35), name=\"weights\")\n",
    "biases = tf.Variable(tf.zeros([number_of_neurons]), name=\"biases\")\n",
    "\n",
    "noise = epsilon*tf.random_normal([batch_size,dimensionality])\n",
    "logits = tf.matmul(tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(tf_data_x+noise, B), biases)), w)\n",
    "out_loss = -tf.reduce_mean(tf.multiply(logits, tf_data_y)) + tf.reduce_mean(tf.math.log(1.0+tf.math.exp(logits)))\n",
    "\n",
    "sigma_prime = tf.multiply(1.0 - tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(tf_data_second, B), biases)), \n",
    "                       tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(tf_data_second, B), biases)))\n",
    "multiply = tf.stack([BATCH_SIZE_2, 1]) \n",
    "w_M_times = tf.tile(tf.transpose(w), multiply)\n",
    "grad_psi = tf.matmul(tf.multiply(w_M_times, sigma_prime), tf.transpose(B))\n",
    "\n",
    "new_part = grad_psi\n",
    "\n",
    "tf_data_sigma_prime = tf.multiply(1.0 - tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(tf_data_second, tf_data_B), tf_data_biases)), \n",
    "                       tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(tf_data_second, tf_data_B), tf_data_biases)))\n",
    "tf_data_w_M_times = tf.tile(tf.transpose(tf_data_w), multiply)\n",
    "tf_data_grad_psi = tf.matmul(tf.multiply(tf_data_w_M_times, tf_data_sigma_prime), tf.transpose(tf_data_B))\n",
    "\n",
    "old_part = tf.matmul(tf_data_grad_psi, tf_data_P)\n",
    "\n",
    "loss2 = out_loss + Lbd*tf.reduce_mean(tf.square(tf.subtract(new_part, old_part)))\n",
    "loss = 1000*loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "3HlgJ8XdjtxY",
    "outputId": "8ec5f3f0-92f7-4712-94c2-e9bff821b918"
   },
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), max_grad_norm)\n",
    "#optimizer = tf.train.GradientDescentOptimizer(new_lr)\n",
    "#target = optimizer.apply_gradients(zip(grads, tvars), global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "target = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YojZRg6lMgur"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1. /(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HLqISRILj5VQ",
    "outputId": "83373660-15c5-4d88-a2da-ec9bcf3a5a56",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for part in range(8):\n",
    "    print(\"Current part is %d\\n\" % part)\n",
    "    train_x = np.concatenate((_train_x[0:(part*65)], _train_x[(part+1)*65:569]), axis=0)\n",
    "    train_y = np.concatenate((_train_y[0:(part*65)], _train_y[(part+1)*65:569]), axis=0)\n",
    "    test_x = _train_x[part*65:(part+1)*65]\n",
    "    test_y = _train_y[part*65:(part+1)*65]\n",
    "    scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "    train_x = scaler.transform(train_x)\n",
    "    test_x = scaler.transform(test_x)\n",
    "\n",
    "    cur_w = np.random.normal(0, 0.35, (number_of_neurons,1))\n",
    "    cur_B =  np.random.normal(0, 0.35, (dimensionality, number_of_neurons))\n",
    "    cur_biases = np.zeros((number_of_neurons))\n",
    "    cur_P = np.zeros((dimensionality, dimensionality)) \n",
    "    O = np.zeros((dimensionality, k)) \n",
    "    cur_iter = 0\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch %d\" %(epoch))\n",
    "        for iteration in range(num_iters):\n",
    "            offset = (cur_iter % 9)*BATCH_SIZE\n",
    "            sample_x = np.reshape(train_x[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, dimensionality))\n",
    "            sample_y = np.reshape(train_y[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, 1))\n",
    "            sess.run(target, feed_dict={tf_data_x: sample_x, tf_data_y: sample_y, \n",
    "                                  tf_data_w: cur_w, tf_data_B: cur_B, tf_data_biases: cur_biases,\n",
    "                                  tf_data_P: cur_P, Lbd: Lambda, batch_size: BATCH_SIZE})\n",
    "            cur_iter = cur_iter+1\n",
    "        reses = []\n",
    "        outes = []\n",
    "        for i in range(9):\n",
    "            offset = (cur_iter % 9)*BATCH_SIZE\n",
    "            sample_x = np.reshape(train_x[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, dimensionality))\n",
    "            sample_y = np.reshape(train_y[offset:(offset+BATCH_SIZE)], (BATCH_SIZE, 1))\n",
    "            res, out = sess.run([loss, out_loss], feed_dict={tf_data_x: sample_x, tf_data_y: sample_y, \n",
    "                                  tf_data_w: cur_w, tf_data_B: cur_B, tf_data_biases: cur_biases,\n",
    "                                  tf_data_P: cur_P, Lbd: Lambda, batch_size: BATCH_SIZE})\n",
    "            reses.append(res)\n",
    "            outes.append(out)\n",
    "            cur_iter = cur_iter+1\n",
    "        print (\"Iter %d: loss: %.4f log-likelihood: %.4f\\n\" %(cur_iter, np.mean(np.array(reses)), np.mean(np.array(outes))))\n",
    "        lib.plot.plot('train cost', np.mean(np.array(reses)))\n",
    "        lib.plot.plot('log-likelihood', np.mean(np.array(outes)))\n",
    "\n",
    "        cur_w, cur_B, cur_biases = sess.run([w, B, biases], feed_dict={})    \n",
    "        third_x = np.random.normal(0, 0.35, (M,dimensionality))\n",
    "        third_sigma_prime = np.multiply(1.0-logistic.cdf(np.add(np.matmul(third_x, cur_B), np.tile(np.reshape(cur_biases,[1, number_of_neurons]),(M,1)))), \n",
    "                           logistic.cdf(np.add(np.matmul(third_x, cur_B), np.tile(np.reshape(cur_biases,[1, number_of_neurons]),(M,1)))))\n",
    "        third_w_M_times = np.tile(np.reshape(cur_w,[1, number_of_neurons]), (M,1))\n",
    "        third_grad_psi = np.matmul(np.multiply(third_w_M_times, third_sigma_prime), np.transpose(cur_B))\n",
    "        \n",
    "        u, s, vh = np.linalg.svd(np.transpose(third_grad_psi), full_matrices=True)\n",
    "        O = u[:,0:k:1]\n",
    "        print(s)\n",
    "        cur_P = np.matmul(O, np.transpose(O))\n",
    "        tvr = 1-np.sum(np.multiply(s[0:k],s[0:k]))/np.sum(np.multiply(s,s))\n",
    "        lib.plot.plot('total variance to retain', tvr)\n",
    "        lib.plot.tick()\n",
    "        lib.plot.flush()\n",
    "    reduced_train_x = np.matmul(train_x, O)\n",
    "    reduced_test_x = np.matmul(test_x, O)\n",
    "    clf = LogisticRegression(random_state=0).fit(reduced_train_x, train_y)\n",
    "    score = clf.score(reduced_test_x, test_y)\n",
    "    scores.append(score)\n",
    "    print (\"Part %d: rate on test %.4f\\n\" %(part, score))\n",
    "    test_rgb = []\n",
    "    for h in test_y:\n",
    "        if h == 1:\n",
    "            test_rgb.append('r')\n",
    "        else:\n",
    "            test_rgb.append('b')\n",
    "    plt2.close()\n",
    "    plt2.scatter(np.transpose(reduced_test_x)[0], np.transpose(reduced_test_x)[1], alpha=0.2, c=np.array(test_rgb))\n",
    "    plt2.xlabel(\"1st component\")\n",
    "    plt2.ylabel(\"2nd component\")\n",
    "    plt2.show()\n",
    "    sess.close()\n",
    "    print(cur_P)\n",
    "print (\"Average rate on test %.4f\\n\" %(np.mean(np.array(scores))))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "SDR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
